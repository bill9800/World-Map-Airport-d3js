{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web_crawler \n",
    "This is a web-crawler created by Bill Chang, and it is made for getting the flight data from website. \n",
    "\n",
    "The purpose is to collect the data for d3.js visualization project, and it is only for personal practice and education. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the html Using BeautifulSoup\n",
    "\n",
    "import urllib to read url content\n",
    "\n",
    "Note that Python3 does not read the **html** code as a string but as a bytearray, so we need to convert it to one with **decode.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import urllib\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "\n",
    "def create_soup(url):\n",
    "    #url = input('Enter the url -') #enter the url\n",
    "    byte_arrary = urllib.request.urlopen(url)\n",
    "    content_bytes = byte_arrary.read()\n",
    "    html = content_bytes.decode(\"utf8\")\n",
    "    byte_arrary.close()\n",
    "    return BeautifulSoup(html)\n",
    "    #use beautifulsoup to read and get the data we want\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print ('The title of page is: \\n',soup.title.string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find out the data\n",
    "\n",
    "According to the data, we can find that there are two kinds of airplane_schedule.\n",
    "\n",
    "One kind is direct, another is non-direct.\n",
    "Hence, we have to carefully extract the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def week(schedule,day,index):\n",
    "    if schedule[index].find('i') == None:\n",
    "        return 'No'\n",
    "    else:\n",
    "        return 'Yes'\n",
    "\n",
    "def element_get(data):\n",
    "    # crawl the data by tag selector\n",
    "    dep_time = data.select('span.schedules__departure-time')[0].contents[0]\n",
    "    dep_place = data.select('span.schedules__departure-code')[0].contents[0]\n",
    "    arr_time = data.select('span.schedules__arrival-time')[0].contents[0]\n",
    "    arr_place = data.select('span.schedules__arrival-code')[0].contents[0]\n",
    "    #fli_dur = data.select('div.schedules__item.schedules__duration')[0].contents[0].strip()\n",
    "    fli_code = data.select('span.schedules__airline-code')[0].contents[2].strip()\n",
    "    aircraft = data.select('span.schedules__aircraft-code')[0].contents[0]\n",
    "  \n",
    "    try:\n",
    "        alt = 0\n",
    "        wait = data.select('div.schedules__item.schedules__transit-airport')[0].contents[0].strip()\n",
    "    except IndexError:\n",
    "        alt = 1\n",
    "        Mon = None\n",
    "        Tus = None\n",
    "        Wed = None\n",
    "        Thur = None\n",
    "        Fri = None\n",
    "        Sat = None\n",
    "        Sun = None\n",
    "        week_schedule = data.find('div',{'class':\"schedules__item schedules__week\"}).contents\n",
    "\n",
    "        Mon = week(week_schedule,Mon,1)\n",
    "        Tus = week(week_schedule,Tus,3)\n",
    "        Wed = week(week_schedule,Wed,5)\n",
    "        Thur = week(week_schedule,Thur,7)\n",
    "        Fri = week(week_schedule,Fri,9)\n",
    "        Sat = week(week_schedule,Sat,11)\n",
    "        Sun = week(week_schedule,Sun,13)\n",
    "    if alt == 1:\n",
    "        return [fli_code,dep_time,dep_place,arr_time,arr_place,aircraft,[Mon,Tus,Wed,Thur,Fri,Sat,Sun]]\n",
    "    if alt == 0:\n",
    "        return [fli_code,dep_time,dep_place,arr_time,arr_place,aircraft]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def title_get(data):\n",
    "    # function to get the title on the web\n",
    "    title_table = data.select('nav.breadcrumbs.breadcrumbs-schedules-city_to_city.breadcrumbs-schedules')\n",
    "    title_list = title_table[0].select('span[itemprop=\"title\"]')\n",
    "\n",
    "    title_part1 = title_list[2].contents[0]\n",
    "    title_part2 = title_list[3].contents[0]\n",
    "    title_part3 = title_list[4].contents[0].split()[3]\n",
    "    title_part4 = title_list[5].contents[0].strip()\n",
    "\n",
    "    title = '('+title_part1+'-'+title_part2+') '+'åˆ°'+' ('+title_part3+'-'+title_part4+')'\n",
    "    return title\n",
    "\n",
    "def data_dict(soup):\n",
    "    #flight table preprocessing\n",
    "    #title = [title_get(soup)]\n",
    "    table = soup.select('li.schedules__data-list.js-schedule-item')\n",
    "    #table2 = soup.select('li.schedules__data-list.js-schedule-item.schedule__non-direct.js-schedule-non-direct.hide')\n",
    "    #table3 = soup.select('li.schedules__data-list.js-schedule-item.js-schedule-non-direct.hide')\n",
    "    #table4 = soup.select('li.schedules__data-list.js-schedule-item.schedule__non-direct.js-schedule-non-direct-codeshare.hide')\n",
    "\n",
    "    list1 = []\n",
    "    #list2 = []\n",
    "    #list3 = []\n",
    "    #list4 = []\n",
    "    for i in table:\n",
    "        list1.append(element_get(i))\n",
    "    '''\n",
    "    for i in table2:\n",
    "        list2.append(element_get(i))\n",
    "\n",
    "    for i in table3:\n",
    "        list3.append(element_get(i))\n",
    "    \n",
    "    for i in table4:\n",
    "        list4.append([title_get(soup)]+element_get(i))    \n",
    "    ''' \n",
    "    #final_table = []\n",
    "    final_dict = {}\n",
    "    # i not in list2 and i not in list3 and \n",
    "    '''\n",
    "    for i in list1:\n",
    "        if i not in list4:\n",
    "            final_table.append(i)\n",
    "    '''\n",
    "    for i in list1:\n",
    "        if i[0] not in final_dict:\n",
    "            final_dict[i[0]] = i\n",
    "    return final_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output the data to csv\n",
    "\n",
    "When we write the data, remember to add **newline=''** to solve it write one more line in the csv.\n",
    "\n",
    "Use writerows to write many lines in only one method.\n",
    "\n",
    "In open, if we want to append data to old file, use 'a' to replace 'w'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def write_data(csv_name,input_dict):\n",
    "    with open(csv_name,'w',newline='') as myfile:   \n",
    "        wr = csv.writer(myfile,delimiter=',',quoting=csv.QUOTE_ALL)\n",
    "        wr.writerows(input_dict.values())\n",
    "        \n",
    "def test_read_data():\n",
    "    # check the data can be read correctly\n",
    "    new_list=[]\n",
    "    with open ('test.csv','r') as myfile:\n",
    "        rd = csv.reader(myfile,delimiter=',',quoting=csv.QUOTE_ALL)\n",
    "        for row in rd:\n",
    "            new_list.append(row)\n",
    "    return new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "def data_append_csv():\n",
    "    # get website data (the link got from first crawler)\n",
    "    with open('web_list.txt','rb') as f:  # read the url list (get from crawler1)\n",
    "        link_list = pickle.load(f)\n",
    "    \n",
    "    step = 1\n",
    "    total = len(link_list)\n",
    "    \n",
    "    for link in link_list:    \n",
    "        step += 1\n",
    "        percent = round((step/total),3)*100\n",
    "        print ('-',percent,'%-',sep='',end='')\n",
    "        my_soup = create_soup(link)\n",
    "        my_dict = data_dict(my_soup)\n",
    "        write_data(my_dict)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#data_append_csv()\n",
    "test_list = test_read_data()\n",
    "test_dict = {}\n",
    "\n",
    "for inner_list in test_list:\n",
    "    test_dict[inner_list[0]] = inner_list\n",
    "for inner_list in test_list:\n",
    "    if len(inner_list) == 7:\n",
    "        test_dict[inner_list[0]] = inner_list\n",
    "write_data('test2.csv',test_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
